{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import praw\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BEARER_TOKEN from key.json\n",
    "with open('key.json') as f:\n",
    "\tkey = json.load(f)\n",
    "\tbearer_token = key['BEARER_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(search, num_tweets = 5):\n",
    "\t# Create the url\n",
    "\turl = 'https://api.twitter.com/1.1/search/tweets.json?result_type=popular&q='+ search + '&max_results=' + str(num_tweets)\n",
    "\t# Create request headers\n",
    "\theaders = {\n",
    "\t\t'authorization': 'Bearer ' + bearer_token,\n",
    "\t\t'user-agent': 'v2FilteredStreamPython',\n",
    "\t}\n",
    "\t# Make the request\n",
    "\tresponse = requests.get(url, headers=headers)\n",
    "\treturn response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wheels_json = get_tweets('wheel', 100)\n",
    "doors_json = get_tweets('door', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to json file\n",
    "with open('data/twitter/wheels.json', 'w') as f:\n",
    "\tjson.dump(wheels_json, f)\n",
    "with open('data/twitter/doors.json', 'w') as f:\n",
    "\tjson.dump(doors_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Build reddit object\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"r3dlLRYKcBN9-JRoyvi1Xw\",\n",
    "    client_secret=\"YfKwRqhLjS1darC2TL5QnrzDe0S_sQ\",\n",
    "    user_agent=\"hiiiiiiiiii\",\n",
    ")\n",
    "\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wheels_reddit = reddit.subreddit(\"all\").search(\"wheel\", limit=1000)\n",
    "doors_reddit = reddit.subreddit(\"all\").search(\"door\" , limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to csv file\t\n",
    "with open('data/reddit/wheels_reddit.csv', 'w') as f:\n",
    "\tf.write('\"title\"\\n')\n",
    "\tfor submission in wheels_reddit:\n",
    "\t\tf.write('\"' + submission.title.replace('\"', '\"\"') + '\"\\n')\n",
    "  \n",
    "with open('data/reddit/doors_reddit.csv', 'w') as f:\n",
    "\tf.write('\"title\"\\n')\n",
    "\tfor submission in doors_reddit:\n",
    "\t\tf.write('\"' + submission.title.replace('\"', '\"\"') + '\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_data(search, num_posts = 5):\n",
    "\t# Create the url for the search also get the posts text\n",
    "\turl = 'https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch=' + search + '&format=json&srlimit=' + str(num_posts)\n",
    "\t# Make the request\n",
    "\tresponse = requests.get(url)\n",
    "\treturn response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wheels_wiki = get_wikipedia_data('wheels', 100)\n",
    "doors_wiki = get_wikipedia_data('doors', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wiki/wheels_wiki.json', 'w') as f:\n",
    "\tjson.dump(wheels_wiki, f)\n",
    "with open('data/wiki/doors_wiki.json', 'w') as f:\n",
    "\tjson.dump(doors_wiki, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pages from wikipedia\n",
    "for page in wheels_wiki['query']['search']:\n",
    "\turl = 'https://en.wikipedia.org/wiki/' + page['title']\n",
    "\tresponse = requests.get(url)\n",
    "\tsoup = BeautifulSoup(response.text, 'html.parser')\n",
    "\tfile_name = 'data/wiki/wheels_wiki_pages/' + page['title'] + '.txt'\n",
    "\t# Make file name compatible with windows\n",
    "\tfile_name = file_name.replace(' ', '_').replace(':', '_').replace('\\\\', '_').replace('\"', '_').replace('?', '_').replace('*', '_').replace('|', '_').replace('<', '_').replace('>', '_')\n",
    "\twith open(file_name, 'w') as f:\n",
    "\t\tf.write(soup.get_text())\n",
    "\ttime.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in doors_wiki['query']['search']:\n",
    "\turl = 'https://en.wikipedia.org/wiki/' + page['title']\n",
    "\tresponse = requests.get(url)\n",
    "\tsoup = BeautifulSoup(response.text, 'html.parser')\n",
    " \n",
    "\tfile_name = 'data/wiki/doors_wiki_pages/' + page['title'] + '.txt'\n",
    "\tfile_name = file_name.replace(' ', '_').replace(':', '_').replace('\\\\', '_').replace('\"', '_').replace('?', '_').replace('*', '_').replace('|', '_').replace('<', '_').replace('>', '_')\n",
    "\twith open(file_name, 'w') as f:\n",
    "\t\tf.write(soup.get_text())\n",
    "\ttime.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_twitter = gensim.downloader.load(\"glove-twitter-200\")\n",
    "model_twitter.save(\"glove_twitter_200.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wiki = gensim.downloader.load(\"glove-wiki-gigaword-300\")\n",
    "model_wiki.save(\"glove_wiki_gigaword_300.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (/home/gabe/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1204175)",
      "at /home/gabe/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (/home/gabe/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223212)",
      "at v.dispose (/home/gabe/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1216694)",
      "at /home/gabe/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533674",
      "at t.swallowExceptions (/home/gabe/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:913059)",
      "at dispose (/home/gabe/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533652)",
      "at t.RawSession.dispose (/home/gabe/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:537330)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "model_google = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "model_google.save(\"word2vec_google_news_300.model\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
